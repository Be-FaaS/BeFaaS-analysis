{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Everything is set up.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import json\n",
    "import befaas.logentry as le\n",
    "from befaas.fileutil import log_entry_iterator\n",
    "\n",
    "sns.set()\n",
    "\n",
    "filepath = \"../dumps/\"\n",
    "dump = \"dump_streaming_aws.json\"\n",
    "outFile = \"../dumps/dump_streaming_aws_u.json\"\n",
    "print(\"Everything is set up.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Include dump dump_streaming_aws.json ...\n",
      "Iterate over ../dumps/dump_streaming_aws.json ...\n",
      "(I)  processed 10000 entries\n",
      "(I)  processed 20000 entries\n",
      "(I)  processed 30000 entries\n",
      "(I)  processed 40000 entries\n",
      "(I)  processed 50000 entries\n",
      "(I)  processed 60000 entries\n",
      "(I)  processed 70000 entries\n",
      "(I)  processed 80000 entries\n",
      "(I)  processed 90000 entries\n",
      "(I)  processed 100000 entries\n",
      "(I)  processed 110000 entries\n",
      "(I)  processed 120000 entries\n",
      "(I)  processed 130000 entries\n",
      "(I)  processed 140000 entries\n",
      "(I)  processed 150000 entries\n",
      "(I)  processed 160000 entries\n",
      "(I)  processed 170000 entries\n",
      "(I)  processed 180000 entries\n",
      "(I)  processed 190000 entries\n",
      "(I)  processed 200000 entries\n",
      "(I)  processed 210000 entries\n",
      "Writing to ../dumps/dump_streaming_aws_u.json\n",
      "Iterate over ../dumps/dump_streaming_aws.json ...\n",
      "(II) processed 10000 entries\n",
      "(II) processed 20000 entries\n",
      "(II) processed 30000 entries\n",
      "(II) processed 40000 entries\n",
      "(II) processed 50000 entries\n",
      "(II) processed 60000 entries\n",
      "(II) processed 70000 entries\n",
      "(II) processed 80000 entries\n",
      "(II) processed 90000 entries\n",
      "(II) processed 100000 entries\n",
      "(II) processed 110000 entries\n",
      "(II) processed 120000 entries\n",
      "(II) processed 130000 entries\n",
      "(II) processed 140000 entries\n",
      "(II) processed 150000 entries\n",
      "(II) processed 160000 entries\n",
      "(II) processed 170000 entries\n",
      "(II) processed 180000 entries\n",
      "(II) processed 190000 entries\n",
      "(II) processed 200000 entries\n",
      "(II) processed 210000 entries\n",
      "Found and removed 34757 duplicates.\n"
     ]
    }
   ],
   "source": [
    "print(f\"Include dump {dump} ...\")\n",
    "# Count entries to get some progress feedback\n",
    "entries = 0\n",
    "contexts = {}\n",
    "otherUserAgentContextList = []\n",
    "\n",
    "for entry in log_entry_iterator(filepath + dump):\n",
    "    entries = entries + 1\n",
    "    if (entries % 10000 == 0):\n",
    "        print(f\"(I)  processed {entries} entries\")\n",
    "\n",
    "    logentry = le.cast_log_type(entry)\n",
    "    if isinstance(logentry, le.RequestLog):\n",
    "        userAgent = logentry.event['request']['headers']['user-agent']\n",
    "        if not str(userAgent).startswith(\"node-fetch\") | str(userAgent).startswith(\"Artillery\"):\n",
    "            print(f\"Other user agent, will filter: {userAgent}\")\n",
    "            otherUserAgentContextList.append(logentry.context_id)\n",
    "\n",
    "entries = 0\n",
    "duplicates = 0\n",
    "print(f\"Writing to {outFile}\")\n",
    "with open(outFile, \"w\") as jsfile:\n",
    "    jsfile.write(\"[\")\n",
    "    initial = True\n",
    "\n",
    "    for entry in log_entry_iterator(filepath + dump):\n",
    "        entries = entries + 1\n",
    "        entryOK = True\n",
    "        if (entries % 10000 == 0):\n",
    "            print(f\"(II) processed {entries} entries\")\n",
    "\n",
    "        logentry = le.cast_log_type(entry)\n",
    "        id = logentry.context_id\n",
    "\n",
    "        if id in otherUserAgentContextList:\n",
    "            # Wrong user agent\n",
    "            print(f\"filtered context {id}\")\n",
    "            entryOK = False\n",
    "            continue\n",
    "\n",
    "        # Create context if there isn't one\n",
    "        if not (id in contexts):\n",
    "            contexts[id] = []\n",
    "\n",
    "        for checkEntry in contexts[id]:\n",
    "            if str(checkEntry).strip() == str(entry).strip():\n",
    "                # found duplicate\n",
    "                duplicates += 1\n",
    "                entryOK = False\n",
    "                #print(f\"filtered a duplicate for ctx {id}\")\n",
    "                continue\n",
    "\n",
    "        if (entryOK):\n",
    "            contexts[id].append(entry)\n",
    "            #write entry to file\n",
    "            jsfile.write((\"\" if initial else \",\\n\") + json.dumps(entry))\n",
    "            initial = False\n",
    "\n",
    "    jsfile.write(\"]\")\n",
    "\n",
    "print(f\"Found and removed {duplicates} duplicates.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}